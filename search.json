[
  {
    "objectID": "notebooks/s1_lora_trump.html",
    "href": "notebooks/s1_lora_trump.html",
    "title": "",
    "section": "",
    "text": "!pip install -q transformers datasets peft accelerate bitsandbytes"
  },
  {
    "objectID": "notebooks/s1_lora_trump.html#train",
    "href": "notebooks/s1_lora_trump.html#train",
    "title": "",
    "section": "Train",
    "text": "Train\n\nimport torch\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, PeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nfrom copy import deepcopy\n\ntorch.random.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7fe5ae6cc250&gt;\n\n\n\n# Load the Trump Tweets dataset\ndataset = load_dataset(\"yunfan-y/trump-tweets-cleaned\")\n\n# Load the tokenizer and the base model\nmodel_name = \"unsloth/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n)\n\n\n# Define a function for generating text\ndef generate_example(prompt, model, tokenizer, max_length=50):\n    model.eval()  # Убедитесь, что модель в режиме генерации\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.95\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove retweets\n    tweet = re.sub(r'^RT\\s+', '', tweet)\n    # Remove user @ references and '#' from hashtags\n    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n    # Remove special characters and numbers\n    # tweet = re.sub(r'[^A-Za-z\\d\\s]', '', tweet)\n    # Convert to lowercase\n    return tweet.strip()\n\n# Preprocessing the data\ndef preprocess_function(examples):\n    tweets = examples[\"text\"]  # Use \"text\" instead of \"content\"\n    inputs = [f\"You are Donald Trump writing a tweet about politics. Your tweet: {clean_tweet(tweet)}\" for tweet in tweets]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n    \ntokenizer.pad_token = tokenizer.eos_token\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n# Split the dataset into training and validation sets\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n\n# Generate text before training\nprint(\"=== Text Generation Before Training ===\")\nprompt = \"Immigrants\"\nprint(generate_example(prompt, model, tokenizer))\n\n# Prepare LoRA configuration\nlora_config = LoraConfig(\n    r=64,  # Rank of the LoRA update matrices\n    lora_alpha=32,  # LoRA scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n    lora_dropout=0.1,  # Dropout rate for LoRA\n    bias=\"none\",  # Do not train biases\n    task_type=\"CAUSAL_LM\"  # Task type for causal language modeling\n)\n\n# Wrap the model with PEFT\npeft_model = get_peft_model(model, lora_config)\noutput_dir = \"./trump_lora\"\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=output_dir,  # Directory for saving the model\n    eval_strategy=\"steps\",  # No evaluation dataset\n    logging_steps=10,\n    save_strategy=\"steps\",\n    save_steps=10,\n    learning_rate=3e-4,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=4,\n    num_train_epochs=1,\n    max_steps=100,\n    weight_decay=0.01,\n    gradient_accumulation_steps=16,\n    warmup_steps=100,\n    logging_dir=\"./logs\",\n    fp16=True,\n    report_to='none',\n)\n\n\n# Define the Trainer with eval_dataset\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\npeft_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Generate text after training\nprint(\"=== Text Generation After Training ===\")\nprint(generate_example(prompt, peft_model, tokenizer))\n\nprint(\"Model fine-tuned and saved!\")\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n=== Text Generation Before Training ===\nImmigrants and the American Dream: The Political and Economic Legacy of Early New York\nImmigrants and the American Dream: The Political and Economic Legacy of Early New York by Anthony F. C. Wallace\nEnglish | February 3, 199\n\n\n\n    \n      \n      \n      [100/100 14:01, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n10\n100.107800\n4.980176\n\n\n20\n32.612900\n0.334947\n\n\n30\n5.274200\n0.308010\n\n\n40\n4.525100\n0.257844\n\n\n50\n3.852100\n0.193377\n\n\n60\n2.864500\n0.175147\n\n\n70\n2.653900\n0.171719\n\n\n80\n2.623700\n0.169075\n\n\n90\n2.646900\n0.167437\n\n\n100\n2.612200\n0.165744\n\n\n\n\n\n\nRepo card metadata block was not found. Setting CardData to empty.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n=== Text Generation After Training ===\nImmigrants’ rights advocates are worried that changes to immigration policies are making it harder for people to seek asylum in the United States. And those changes have also made it harder to prove that someone is eligible for asylum, according to the experts interviewed by\nModel fine-tuned and saved!"
  },
  {
    "objectID": "notebooks/s1_lora_trump.html#inference",
    "href": "notebooks/s1_lora_trump.html#inference",
    "title": "",
    "section": "Inference",
    "text": "Inference\nSeems like when we load fine-tuned model, it overwrites original model, so restart notebook just in case before inference\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\ni = 6 # device number to use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n\n\nimport torch\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, PeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nfrom copy import deepcopy\n\ntorch.random.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7f4c68630310&gt;\n\n\n\n# Load the Trump Tweets dataset\ndataset = load_dataset(\"yunfan-y/trump-tweets-cleaned\")\n\n# Load the tokenizer and the base model\nmodel_name = \"unsloth/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n)\n\n\n# Define a function for generating text\ndef generate_example(prompt, model, tokenizer, max_length=50):\n    model.eval()  # Убедитесь, что модель в режиме генерации\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.95\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove retweets\n    tweet = re.sub(r'^RT\\s+', '', tweet)\n    # Remove user @ references and '#' from hashtags\n    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n    # Remove special characters and numbers\n    # tweet = re.sub(r'[^A-Za-z\\d\\s]', '', tweet)\n    # Convert to lowercase\n    return tweet.strip()\n\n# Preprocessing the data\ndef preprocess_function(examples):\n    tweets = examples[\"text\"]  # Use \"text\" instead of \"content\"\n    inputs = [f\"You are Donald Trump writing a tweet about politics. Your tweet: {clean_tweet(tweet)}\" for tweet in tweets]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n    \ntokenizer.pad_token = tokenizer.eos_token\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n# Split the dataset into training and validation sets\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n\nOriginal model\n\nfrom torch.utils.data import DataLoader\nimport math\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Evaluate both models on the evaluation dataset\nprint(\"=== Comparing Model Performance on Evaluation Dataset ===\")\n\n# Evaluate original model\nmodel.eval()\neval_loss = 0\neval_steps = 0\neval_data_loader = DataLoader(eval_dataset, batch_size=4, collate_fn=lambda x: {k: torch.tensor([d[k] for d in x]).to(model.device) for k in x[0].keys()})\nwith torch.no_grad():\n    for batch in eval_data_loader:\n        outputs = model(**{k: v for k, v in batch.items() if k != 'labels'}, labels=batch['labels'])\n        eval_loss += outputs.loss.item()\n        eval_steps += 1\noriginal_perplexity = math.exp(eval_loss / eval_steps)\nprint(f\"Original Model Perplexity: {original_perplexity:.2f}\")\nprint(f\"Original Model Loss: {eval_loss:.2f}\")\n\n=== Comparing Model Performance on Evaluation Dataset ===\nOriginal Model Perplexity: 732.55\nOriginal Model Loss: 554.11\n\n\n\ntest_prompts = [\n    \"I think, Donald Trump\",\n    \"I think, Barack Obama\",\n    \"I think, Joe Biden\",\n    \"I think, the Democrats are\",\n    \"We need to\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(3):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump will be the next President of the United States. There is a chance that he won’t but if he doesn’t then I’ll be very disappointed. To me, it’s the only way that America can survive.\nAmerica\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, the newly elected 45th President of the United States has just pulled a fast one on us. I think, he is going to be a great President. I’m not going to hold my breath, however,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump is the most intelligent President the US ever had, and the current one, Barack Obama is the most stupid one. I know it is hard to believe, but it is a fact. Trump is the most intelligent, because\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama would be the first African American president, who will make history in the USA, but he is not the first Black president. That is actually George Washington Carver, the inventor and scientist who was the first African American president\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama will win the US election. He has a great chance, since the opposition, John McCain, has made a big mistake. He has promised to give the same tax reductions to people with a high income as to those with\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a really nice man. I mean, his wife is really nice too. I mean, I can't think of a better word to describe them. And I like his daughter Malia. She seems really sweet.\n\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden, the vice president, is the strongest candidate that we’ve had in 25 years.\nIf you want to do more than just talk about how terrible you think Donald Trump is, you have to offer a specific vision of\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is the first president in recent history who has the potential to be reelected to the presidency. The reason is he is a person with a sense of humor, he is a good orator, and he has a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden’s political career has been at the brink of collapse since he was elected Vice President. It seems to me that his political career is going to be over sooner than later. The political environment is changing. His party is losing\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are in trouble this time. They are the ones who have been saying that the Republicans can not be trusted. I think that they have the right to be scared. They are getting very, very, very, very,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are now afraid of losing their super-majority in the House and Senate. And they are now trying to get the votes by offering amnesty to illegal immigrants.\nThe Democrats are now trying to pass amnesty, but the Republicans are\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are going to try and impeach Trump for the things they said they’d do. But it didn’t work in 1998 and it won’t work now. They tried to impeach Bush for lying about Iraq,\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to start doing things differently. We must begin to take care of our planet if we want to pass it on to our children. The way we travel is contributing to global warming and pollution. We need to look at ways to make our\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to think hard about the impact of our choices in life.\nThe life of a 6 year old girl was saved by her mother’s choice to make a different kind of dinner.\nWe need to think hard about the impact of our choices\n---\nWe need to look at the whole picture and get our minds around it. We need to understand what we are up against. We need to be able to see the big picture and be able to make sense of it.\n---\n\n\n\n\ntest_prompts = [\n    \"The United States\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has been working with its partners in the United Nations Security Council to impose sanctions on the Syrian government for its alleged use of chemical weapons in April against civilians in the town of Khan Sheikhoun in Idlib governorate. The US,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States government is a massive employer, and is always looking for qualified candidates to fill a wide variety of open employment positions in locations across the country. Below you’ll find a Qualification Summary for an active, open job listing from the Department\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States of America is a federal country made up of fifty states and one federal district. It is the world’s largest industrial, military and economic power, and is the leading global superpower.\nThe United States of America is the third largest\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Postal Service is a government-owned corporation that has been operating under the radar for a long time. It is the sole mail delivery company in the United States and is one of the biggest companies in the world, with more than 1\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is the only country in the world that does not have a national health insurance system. In 2019, 28.5 million people had no health insurance coverage, and in 2021, 37 million people remained uninsured\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is currently ranked No. 28 on the list of the world’s countries by life expectancy at birth. It is estimated that the US will have a population of 325 million in 2015. Currently, there are nearly \n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is home to an estimated 1.2 million refugees, about half of whom arrived after 9/11. For decades, America has welcomed refugees, granting permanent residence to those fleeing persecution and war. But, in recent years\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a nation with a long history of slavery and the legacy of slavery still affects many people today. One such legacy is the practice of slavery in the United States. This practice has been around for centuries and has had a significant impact\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has been home to a number of notable African Americans, including athletes, politicians, entertainers, and military figures. One of the most famous was Jackie Robinson, who broke the MLB color barrier in 1947. Here are \n---\nThe United States and the rest of the world are facing an unprecedented crisis of misinformation. As the COVID-19 pandemic and other crises have unfolded, millions of people have been bombarded with lies and fake news on social media and other platforms. Mis\n---\n\n\n\n\ntest_prompts = [\n    \"Hillary Clinton is a\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a master of the backroom deal. She's not afraid to reach across the aisle to forge a deal that can get something done. Even though it might not be her ideal, she's willing to compromise to get something done.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a hypocrite for calling for tougher laws to stop sexual harassment in the workplace — and she should know better.\nLast week, the former first lady and secretary of state blasted a federal judge who released the record of a settlement between President\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a former U.S. Secretary of State, a former U.S. Senator from New York, and the 67th U.S. Secretary of State. She served as the First Lady of the United States from 1993 to\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a former United States Senator from New York, and the First Lady of the United States. She is a former U.S. Secretary of State and a presidential candidate in the 2008 Democratic primaries. She is running for the Democratic\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a racist. The former secretary of state and presidential hopeful has a long record of promoting racially charged stereotypes that were designed to undermine the civil rights movement.\nBut a recent poll shows that she’s also a racist among her own party.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a liar. She lies like a rug. The evidence is clear and strong. She lies and then she lies some more. That’s why she’s not fit to be president, but that’s not her only problem. She’s\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a terrible liar. It’s not that she’s a terrible person, it’s that she’s a terrible politician. She should have known better than to even try to lie her way out of this one. She’s so good at\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a \"warmonger\" and should not be president, Donald Trump said on Monday, setting up a likely confrontation with the former secretary of state as the presidential election looms.\nIn a speech at the New York Hilton hotel on\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a dangerous woman\nby Mark Steyn / September 11, 2015 / Leave a comment\nHillary Clinton and her husband Bill Clinton at the 2015 Clinton Global Initiative © Alain Jocard/AFP/Getty Images\nThe\n---\nHillary Clinton is a former U.S. Secretary of State and former First Lady of the United States. A graduate of Yale University and Yale Law School, she served as U.S. Senator for New York from 2001 to 2009,\n---\n\n\n\n\n\nLora model\nMost of the prompts give rather the same results. Probably, I could not find any good prompts for this model. But qualitative results (loss/perplexity on evaluation dataset) speak for itself.\n\n# Evaluate LORA model\noutput_dir = \"./trump_lora\"\npeft_model = PeftModelForCausalLM.from_pretrained(model, output_dir)\npeft_eval_loss = 0\npeft_eval_steps = 0\nwith torch.no_grad():\n    for batch in eval_data_loader:\n        outputs = peft_model(**{k: v for k, v in batch.items() if k != 'labels'}, labels=batch['labels'])\n        peft_eval_loss += outputs.loss.item()\n        peft_eval_steps += 1\npeft_perplexity = math.exp(peft_eval_loss / peft_eval_steps)\nprint(f\"LORA Model Perplexity: {peft_perplexity:.2f}\")\nprint(f\"LORA Model Loss: {peft_eval_loss:.2f}\")\nprint(f\"Perplexity Improvement: {original_perplexity - peft_perplexity:.2f}\")\nprint(f\"Loss improvement: {eval_loss - peft_eval_loss:.2f}\")\n\nLORA Model Perplexity: 1.18\nLORA Model Loss: 13.93\nPerplexity Improvement: 731.37\nLoss improvement: 540.18\n\n\n\ntest_prompts = [\n    \"I think, Donald Trump\",\n    \"I think, Barack Obama\",\n    \"I think, Joe Biden\",\n    \"I think, the Democrats are\",\n    \"We need to\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(3):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, for all his faults, is the most interesting presidential candidate since perhaps William Jennings Bryan. The thing is, I can’t be sure what Trump is thinking. He’s an enigma.\nNow, I don’t want\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, a lot, about politics, and about the country, and about the world, and about everything else. He is a great thinker, and he thinks about everything. The reason he is successful is because he is a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump just won the Presidential election! He will be a great President for our Country! I’m proud to call myself a Donald Trump fan! He’s smart, a great businessman, a great TV personality and a great President!\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama, has set a dangerous precedent with his latest tweet regarding politics and the press. Let me explain.\nThere are a lot of writers out there who just write about politics, and they don’t understand politics at all. They\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a great man, I don't care what his politics are. He's a great man and we need a lot more men like him.\nI'm the best friend you'll ever have. I'm the worst enemy\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a good man. I hope he is reelected as president of the United States. He has done a great job as president of the United States. I hope he will be reelected as president of the\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is a great person and a great politician. He has always been a great president and I think that he has always done a great job of leading the United States and I think that he is a great leader and I think\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is one of the best presidents the United States of America has ever had. He is the man who brought down the Berlin Wall and the Iron Curtain. He helped end the Cold War and the threat of nuclear war.\nHe\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is the best for America. He is a very intelligent and kind person, who really cares about others. He knows what is going on in America and what the best for America is. He has a lot of experience in\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are being too clever by half. They are playing a game of “chicken” with the Republicans. The Republicans have not played the game of “chicken” very well. The Democrats have not played the game of\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are scared of Trump because he is a very strong leader, unlike Obama. He has a lot of political skills and I hope he wins the next election.\nI think, the Democrats are scared of Trump because he is a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are a better party than the Republicans. I would say the Republicans are a joke. And the only thing they really have going is their big money. I think they have the most corrupt politicians in the world. I just\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to protect our environment and the lives of all creatures on this planet. We must protect our planet, our natural resources, our environment, and the animals that inhabit it. We must all take action to save the Earth.\nWe all have\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to have a plan on how to deal with a crisis and the best place to start is with a crisis plan. It is important to know what type of crisis you might be facing and to have a plan in place that can be used\n---\nWe need to protect the environment from pollution by plastic. But what about the plastic inside of us? What is your body made of? Your body is made of different parts and they all serve a different purpose. We need these parts to stay alive\n---\n\n\n\n\ntest_prompts = [\n    \"The United States\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Supreme Court has ruled that federal law does not grant standing to individuals who simply make claims that they were damaged by another party’s actions. In Spokeo, Inc. v. Robins, the Court explained that “standing is\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States’ trade deficit with China was $34.9 billion in the third quarter of 2017, the Commerce Department reported on Tuesday. The trade deficit with China was $375.2 billion in 2017, up from $\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Air Force announced today that the B-52H Stratofortress has earned a top spot on the prestigious 2017 Time Magazine’s Top 100 of the World’s Greatest Machines list. The venerable long-range bomber took\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Department of State has announced that it is accepting applications for the Diversity Visa Lottery program for the year 2020. The DV-2020 lottery will offer 55,000 visas for immigrants to the USA who would like to live\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is set to host its first ever esports World Cup in Dallas, Texas at the Dallas Convention Center from August 2nd to 4th. The event will be held in the same venue as the International eSports Federation (iSF\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a nation of immigrants, with people from around the world coming to America in search of a better life. However, the process of immigrating to the United States can be complex and daunting, especially for those who are not familiar\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has become a global superpower as the result of the country's abundant natural resources, efficient markets, and large and motivated population. The United States has a great deal of natural resources, including forests, minerals, oil, and gas\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a country of immigrants, and immigrants have always been an important part of our culture. This is why we are such a diverse and multicultural nation, with people from all over the world coming to make their lives here. If you\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States of America is a federal republic of 50 states and the District of Columbia (DC). The 50 states are the principal political divisions of the country. The District of Columbia is a federal district and is not a state. There\n---\nThe United States is home to many unique attractions, including the Statue of Liberty and the Golden Gate Bridge. The country is also well known for its delicious cuisine, which ranges from the iconic burgers and pizza to the diverse range of international dishes. From\n---\n\n\n\n\ntest_prompts = [\n    \"Hillary Clinton is a\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a bad role model for our children.\nThis should be the biggest concern for all parents with children of voting age, but especially parents who support a candidate for President.\nI am not a Clinton supporter, but I do believe that the\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a good writer. She’s a good speaker. She’s a good politician. I don’t believe she’s a good person. And if you want to be president you need to be a good person.\nI don’t know if\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a woman of many contradictions: one of the smartest in the world, and yet a political novice with little experience running for office; a brilliant negotiator, and yet a poor campaigner who has never been able to rally her\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a liar, a hypocrite, and a crook. She has been for years. She is a disgusting person who does not deserve the office of President. She is a weakling who would not be a good leader, she\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a real estate investor and owner of a company with a multimillion-dollar portfolio of real estate investments. Her investments include a building in New York City and in Florida. She has also invested in businesses in the United States and abroad.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a terrible candidate for president of the United States. She is bad on domestic issues. She is bad on foreign issues. She is bad on economics. And she is bad on the Constitution. She would be the worst president we have\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a self-serving politician who uses her position to sell access and influence to donors and special interests, including foreign governments. She is the most corrupt and untrustworthy candidate ever to seek the presidency. She is a liar who has been\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a true fighter and one of the most extraordinary women of all time. Her book Living History is a great read. I’m sure she will continue to do great things for America and the world.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a major liar and manipulator who would do anything to gain power.\nHer policies are a disaster for America and the world. Her agenda is a threat to our freedoms and the Constitution.\nShe has a history of dishonesty and corruption\n---\nHillary Clinton is a lying scum sucking whore who is only interested in selling her ass for a buck and lining her pockets. I don't know why she is even allowed to run for office. She is a disgrace to the country and should be\n---"
  },
  {
    "objectID": "notebooks/s4_benchmarx_convex.html",
    "href": "notebooks/s4_benchmarx_convex.html",
    "title": "Support Vector Machine",
    "section": "",
    "text": "!pip install numpy matplotlib jax scipy scikit-optimize ucimlrepo optax\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\nArray([  0, 228], dtype=uint32)\n\n\n\n@jit\ndef logistic_loss(w, X, y, mu=1):\n    m, n = X.shape\n    return jnp.sum(jnp.logaddexp(0, -y * (X @ w))) / m + mu / 2 * jnp.sum(w**2)\n\ndef generate_problem(m=1000, n=300, mu=1):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//2, random_state=0)\n    X = jnp.array(X)\n    y = jnp.array(y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n    return X_train, y_train, X_test, y_test\n\ndef compute_optimal(X, y, mu):\n    w = cp.Variable(X.shape[1])\n    objective = cp.Minimize(cp.sum(cp.logistic(cp.multiply(-y, X @ w))) / len(y) + mu / 2 * cp.norm(w, 2)**2)\n    problem = cp.Problem(objective)\n    problem.solve()\n    return w.value, problem.value\n\n@jit\ndef compute_accuracy(w, X, y):\n    # Compute predicted probabilities using the logistic (sigmoid) function\n    preds_probs = jax.nn.sigmoid(X @ w)\n    # Convert probabilities to class predictions: -1 if p &lt; 0.5, else 1\n    preds = jnp.where(preds_probs &lt; 0.5, 0, 1)\n    # Calculate accuracy as the average of correct predictions\n    accuracy = jnp.mean(preds == y)\n    return accuracy\n\n\n\n# @jit\ndef compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu):\n    f = lambda w: logistic_loss(w, X_train, y_train, mu)\n    metrics = {\n        \"f_gap\": [jnp.abs(f(x) - f_star) for x in trajectory],\n        \"x_gap\": [jnp.linalg.norm(x - x_star) for x in trajectory],\n        \"time\": times,\n        \"train_acc\": [compute_accuracy(x, X_train, y_train) for x in trajectory],\n        \"test_acc\": [compute_accuracy(x, X_test, y_test) for x in trajectory],\n    }\n    return metrics\n\ndef gradient_descent(w_0, X, y, learning_rate=0.01, num_iters=100, mu=0):\n    trajectory = [w_0]\n    times = [0]\n    w = w_0\n    f = lambda w: logistic_loss(w, X, y, mu)\n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_val = grad(f)(w)\n        if learning_rate == \"linesearch\":\n            # Simple line search implementation\n            phi = lambda alpha: f(w - alpha*grad_val)\n            result = minimize_scalar(fun=phi, \n                                     bounds=(1e-3, 2e2)\n                              )\n            step_size = result.x\n        else:\n            step_size = learning_rate\n        w -= step_size * grad_val\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    return trajectory, times\n\ndef run_experiments(params):\n    mu = params[\"mu\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X_train, y_train, X_test, y_test = generate_problem(m, n, mu)\n    n_features = X_train.shape[1]  # Number of features\n    params[\"n_features\"] = n_features\n    \n    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n_features, ))\n    x_star, f_star = compute_optimal(X_train, y_train, mu)\n\n    for method in methods:\n        learning_rate = method[\"learning_rate\"]\n        iterations = method[\"iterations\"]\n        trajectory, times = gradient_descent(x_0, X_train, y_train, learning_rate, iterations, mu)\n        label = method[\"method\"] + \" \" + str(learning_rate)\n        results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n\n    return results, params\n\ndef plot_results(results, params):\n    plt.figure(figsize=(11, 5))\n    mu = params[\"mu\"]\n    \n    if mu &gt; 1e-2:\n        plt.suptitle(f\"Strongly convex binary logistic regression. mu={mu}.\")\n    else:\n        plt.suptitle(f\"Convex binary logistic regression. mu={mu}.\")\n\n    plt.subplot(2, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 5)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['f_gap'])\n    plt.xlabel('Time')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 6)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['x_gap'])\n    plt.xlabel('Time')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 7)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"train_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 8)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"test_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    # Place the legend below the plots\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    # Adjust layout to make space for the legend below\n    filename = \"\"\n    for method, metrics in results.items():\n        filename += method\n    filename += f\"_{mu}.pdf\"\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.savefig(filename)\n    plt.show()\n\n&lt;&gt;:133: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:165: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:133: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:165: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_87087/2871042674.py:133: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_87087/2871042674.py:165: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"mu\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-1,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-1,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\n\n\n\n\n\n\n\nparams = {\n    \"mu\": 1e-1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 1e-1,\n            \"iterations\": 900,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 1.1e-1,\n            \"iterations\": 900,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\n\n\n\n\n\n\n\nSupport Vector Machine\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\nArray([  0, 228], dtype=uint32)\n\n\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\n# Generate a synthetic binary classification dataset\ndef generate_svm_data(m=1000, n=300):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, \n                                       n_informative=n//2, random_state=42)\n    y = 2 * y - 1  # Convert labels to {-1, 1}\n    return X, y\n\n# Solve SVM using convex optimization\ndef compute_svm_optimal(X, y, C=1.0):\n    m, n = X.shape\n    w = cp.Variable(n)\n    b = cp.Variable()\n    slack = cp.Variable(m)\n\n    # SVM objective: minimize 1/2 ||w||^2 + C * sum(slack)\n    objective = cp.Minimize(0.5 * cp.norm(w, 2)**2 + C * cp.sum(slack))\n\n    # Constraints:  y_i (x_i^T w + b) &gt;= 1 - slack_i,  slack_i &gt;= 0\n    constraints = [\n        cp.multiply(y, X @ w + b) &gt;= 1 - slack,\n        slack &gt;= 0\n    ]\n\n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n\n    return w.value, b.value, problem.value\n\n# Hinge loss function\n@jax.jit\ndef hinge_loss(w, b, X, y, C=1.0):\n    margins = y * (X @ w + b)\n    loss = jnp.maximum(0, 1 - margins)\n    return 0.5 * jnp.sum(w**2) + C * jnp.mean(loss)\n\n# Compute accuracy\n@jax.jit\ndef compute_accuracy(w, b, X, y):\n    preds = jnp.sign(X @ w + b)\n    return jnp.mean(preds == y)\n\n# Perform gradient descent for SVM optimization\ndef gradient_descent_svm(w_0, b_0, X, y, learning_rate=0.01, num_iters=100, C=1.0):\n    trajectory = [w_0]\n    times = [0]\n    w, b = w_0, b_0\n    f = lambda w, b: hinge_loss(w, b, X, y, C)\n    \n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_w, grad_b = grad(f, argnums=(0, 1))(w, b)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    \n    return trajectory, times\n\n# Run SVM Experiments\ndef run_experiments_svm(params):\n    C = params[\"C\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X, y = generate_svm_data(m, n)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    # Standardize data\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    w_0 = jax.random.normal(jax.random.PRNGKey(0), (n,))\n    b_0 = 0.0\n\n    w_star, b_star, f_star = compute_svm_optimal(X_train, y_train, C)\n\n    for method in methods:\n        learning_rate = method[\"learning_rate\"]\n        iterations = method[\"iterations\"]\n        trajectory, times = gradient_descent_svm(w_0, b_0, X_train, y_train, learning_rate, iterations, C)\n        label = method[\"method\"] + \" \" + str(learning_rate)\n        results[label] = {\n            \"f_gap\": [jnp.abs(hinge_loss(w, b_0, X_train, y_train, C) - f_star) for w in trajectory],\n            \"x_gap\": [jnp.linalg.norm(w - w_star) for w in trajectory],\n            \"time\": times,\n            \"train_acc\": [compute_accuracy(w, b_0, X_train, y_train) for w in trajectory],\n            \"test_acc\": [compute_accuracy(w, b_0, X_test, y_test) for w in trajectory],\n        }\n\n    return results, params\n\n# Plot Results\ndef plot_results_svm(results, params):\n    plt.figure(figsize=(11, 5))\n    C = params[\"C\"]\n    \n    plt.suptitle(f\"SVM Training Results. C={C}\")\n\n    plt.subplot(2, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 5)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['f_gap'])\n    plt.xlabel('Time')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 6)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['x_gap'])\n    plt.xlabel('Time')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 7)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"train_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Train accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 8)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"test_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Test accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.show()\n\n&lt;&gt;:119: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:149: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:119: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:149: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_86548/3657982053.py:119: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_86548/3657982053.py:149: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"C\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-3,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-3,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments_svm(params)\nplot_results_svm(results, params)\n\n\n\n\n\n\n\n\n\nparams = {\n    \"C\": 0.1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-3,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-3,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments_svm(params)\nplot_results_svm(results, params)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization for ML",
    "section": "",
    "text": "Optimization for ML\n\nCourse for 3rd year students of CS department at HSE university. 1 lecture + 1 seminar per week.\nThe course covers convex, non-convex, continuous optimization topics, especially motivated by problems and applications in Machine Learning. Various topics are covered, from fundamental materials to recent research.\nDescription of the course.\n\nYour browser does not support the video tag.\n\n\nTeam\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Меркулов\n                    \n                    Преподаватель\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Петр Остроухов\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Илья Забара\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Мария Медведева\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Ян Максимов\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Андрей Игнатов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Максим Шуклин\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Динар Саберов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Михаил Конобеев\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Анна Петрова\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Кирилл Королев\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Елена Лыкова\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Мария Розаева\n                    \n                    Ассистентка\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Юрий Пустовалов\n                    \n                    Ассистент\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/s2_brent.html",
    "href": "notebooks/s2_brent.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\n\n# Define two test functions\ndef f1(x):\n    return np.sin(x) + 0.1 * (x ** 2)\n\ndef f2(x):\n    return np.exp(-x**2) + 0.5 * np.cos(3 * x)\n\n# Optimization settings\nmethods = ['brent', 'golden']\nfunctions = [f1, f2]\nfunction_names = [r'$f_1(x) = sin(x) + 0.1 \\cdot x^2$', r'$f_2(x) = exp(-x^2) + 0.5 \\cdot cos(3x)$']\n\n# Store results for plotting\nresults = {}\n\nfor func, func_name in zip(functions, function_names):\n    results[func_name] = {}\n    for method in methods:\n        function_call_values = []\n\n        # Wrap the function to track calls\n        def wrapped_func(x):\n            value = func(x)\n            function_call_values.append(value)\n            return value\n\n        result = minimize_scalar(wrapped_func, method=method)\n\n        # Collect data for plotting\n        results[func_name][method] = {\n            'function_values': function_call_values,\n            'num_calls': len(function_call_values),\n            'x_min': result.x,\n            'fun_min': result.fun\n        }\n\n# Plot results\nfor func_name in function_names:\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\n    # Plot function value vs. iteration number\n    for method in methods:\n        ax.plot(\n            range(len(results[func_name][method]['function_values'])),\n            [fv - results[func_name][method]['fun_min'] for fv in results[func_name][method]['function_values']],\n            label=f'{method.capitalize()} Method'\n        )\n    ax.set_yscale('log')\n    ax.set_title(func_name)\n    ax.set_xlabel('Iteration Number')\n    ax.set_ylabel('Function Value - Optimal Value (log scale)')\n    ax.legend()\n    ax.grid()\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "Занятие 1\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar • ▶️ Youtube • 💿 Скачать\n    \n    Вспоминаем линейную алгебру. Некоторые матричные разложения. Скорость сходимости.\n\n    Занятие 2\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar • ▶️ Youtube • 💿 Скачать\n    \n    Одномерная оптимизация. Неточная одномерная оптимизация. Градиент. Гессиан. Матрично-векторное дифференцирование.\n\n    Занятие 3\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar • ▶️ Youtube • 💿 Скачать\n    \n    Автоматическое дифференцирование. Вычислительный граф.\n\n    Занятие 4\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Выпуклость. Выпуклые множества. Выпуклые функции. Неравенство Йенсена. Сильно выпуклые функции. Условие Поляка - Лоясиевича. Выпуклость нейронных сетей.\n\n    Занятие 5\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Условия оптимальности. Функция Лагранжа. Задачи с ограничениями-равенствами. Задачи с ограничениями-равенствами. Теорема Каруша - Куна - Таккера.\n\n    Занятие 6\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Двойственность.\n\n    Занятие 7\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Задача линейного программирования. Симплекс метод.\n\n    Занятие 8\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости.\n\n    Занятие 9\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Ускоренные градиентные методы. Метод Поляка, Нестерова.\n\n    Занятие 10\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Субградиент. Субдифференциал. Субградиентный спуск. Теоремы сходимости в негладком случае. Особенности работы градиентного метода в практических негладких задачах.\n\n    Занятие 11\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Метод сопряженных градиентов.\n\n    Занятие 12\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Градиентные методы в условных задачах оптимизации - метод проекции градиента. Метод Франк - Вульфа. Идея метода зеркального спуска.\n\n    Занятие 13\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Проксимальный градиентный метод.\n\n    Занятие 14\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Метод Ньютона. Квазиньютоновские методы.\n\n    Занятие 15\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Стохастический градиентный спуск.\n\n    Занятие 16\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Методы редукции дисперсии: SAG, SVRG, SAGA. Адаптивные стохастические градиентные методы.\n\n    Занятие 17\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Обучение нейронных сетей с точки зрения методов оптимизации. Обобщающая способность моделей машинного обучения. Double Descent. Grokking. Mode connectivity.\n\n    Занятие 18\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Вопросы обучения больших моделей. Lars, Lamb. Learning rate schedulers. Warm-up. MultiGPU training.\n\n    Занятие 19\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar\n    \n    Введение в двойственные методы оптимизации. Метод двойственного градиентного подъёма. Метод модифицированной функции Лагранжа. ADMM.\n\n\nNo matching items"
  }
]