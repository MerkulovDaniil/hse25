[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization for ML",
    "section": "",
    "text": "Optimization for ML\n\nCourse for 3rd year students of CS department at HSE university. 1 lecture + 1 seminar per week.\nThe course covers convex, non-convex, continuous optimization topics, especially motivated by problems and applications in Machine Learning. Various topics are covered, from fundamental materials to recent research.\nDescription of the course.\n\nYour browser does not support the video tag.\n\n\nTeam\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –î–∞–Ω–∏–∏–ª –ú–µ—Ä–∫—É–ª–æ–≤\n                    \n                    –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ü–µ—Ç—Ä –û—Å—Ç—Ä–æ—É—Ö–æ–≤\n                    \n                    –°–µ–º–∏–Ω–∞—Ä–∏—Å—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ò–ª—å—è –ó–∞–±–∞—Ä–∞\n                    \n                    –°–µ–º–∏–Ω–∞—Ä–∏—Å—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ú–∞—Ä–∏—è –ú–µ–¥–≤–µ–¥–µ–≤–∞\n                    \n                    –°–µ–º–∏–Ω–∞—Ä–∏—Å—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –Ø–Ω –ú–∞–∫—Å–∏–º–æ–≤\n                    \n                    –°–µ–º–∏–Ω–∞—Ä–∏—Å—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ê–Ω–¥—Ä–µ–π –ò–≥–Ω–∞—Ç–æ–≤\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ú–∞–∫—Å–∏–º –®—É–∫–ª–∏–Ω\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –î–∏–Ω–∞—Ä –°–∞–±–µ—Ä–æ–≤\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ú–∏—Ö–∞–∏–ª –ö–æ–Ω–æ–±–µ–µ–≤\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ê–Ω–Ω–∞ –ü–µ—Ç—Ä–æ–≤–∞\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ö–∏—Ä–∏–ª–ª –ö–æ—Ä–æ–ª–µ–≤\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ï–ª–µ–Ω–∞ –õ—ã–∫–æ–≤–∞\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –ú–∞—Ä–∏—è –†–æ–∑–∞–µ–≤–∞\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç–∫–∞\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      –Æ—Ä–∏–π –ü—É—Å—Ç–æ–≤–∞–ª–æ–≤\n                    \n                    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "–ó–∞–Ω—è—Ç–∏–µ 1\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—Å–ø–æ–º–∏–Ω–∞–µ–º –ª–∏–Ω–µ–π–Ω—É—é –∞–ª–≥–µ–±—Ä—É. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–∞—Ç—Ä–∏—á–Ω—ã–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è. –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 2\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –û–¥–Ω–æ–º–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è. –ù–µ—Ç–æ—á–Ω–∞—è –æ–¥–Ω–æ–º–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è. –ì—Ä–∞–¥–∏–µ–Ω—Ç. –ì–µ—Å—Å–∏–∞–Ω. –ú–∞—Ç—Ä–∏—á–Ω–æ-–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 3\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ. –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 4\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—ã–ø—É–∫–ª–æ—Å—Ç—å. –í—ã–ø—É–∫–ª—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –í—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ô–µ–Ω—Å–µ–Ω–∞. –°–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –£—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞ - –õ–æ—è—Å–∏–µ–≤–∏—á–∞. –ú–∏–Ω–∏–º—É–º—ã –ª–∏–Ω–µ–π–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.\n\n    –ó–∞–Ω—è—Ç–∏–µ 5\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –£—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏. –§—É–Ω–∫—Ü–∏—è –õ–∞–≥—Ä–∞–Ω–∂–∞. –ó–∞–¥–∞—á–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏-—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞–º–∏. –ó–∞–¥–∞—á–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏-—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞–º–∏. –¢–µ–æ—Ä–µ–º–∞ –ö–∞—Ä—É—à–∞ - –ö—É–Ω–∞ - –¢–∞–∫–∫–µ—Ä–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 6\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å.\n\n    –ó–∞–Ω—è—Ç–∏–µ 7\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ó–∞–¥–∞—á–∞ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏–º–ø–ª–µ–∫—Å –º–µ—Ç–æ–¥.\n\n    –ó–∞–Ω—è—Ç–∏–µ 8\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–µ, —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã–µ, PL). –í–µ—Ä—Ö–Ω–∏–µ –∏ –Ω–∏–∂–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 9\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ú–µ—Ç–æ–¥ –ü–æ–ª—è–∫–∞, –ù–µ—Å—Ç–µ—Ä–æ–≤–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 10\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç. –°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª. –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –Ω–µ–≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ. –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–≥–ª–∞–¥–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö.\n\n    –ó–∞–Ω—è—Ç–∏–µ 11\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n\n    –ó–∞–Ω—è—Ç–∏–µ 12\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ —É—Å–ª–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ - –º–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫ - –í—É–ª—å—Ñ–∞. –ò–¥–µ—è –º–µ—Ç–æ–¥–∞ –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 13\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥.\n\n    –ó–∞–Ω—è—Ç–∏–µ 14\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞. –ö–≤–∞–∑–∏–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 15\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫.\n\n    –ó–∞–Ω—è—Ç–∏–µ 16\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –ú–µ—Ç–æ–¥—ã —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏: SAG, SVRG, SAGA. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 17\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –û–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. Double Descent. Grokking. Mode connectivity.\n\n    –ó–∞–Ω—è—Ç–∏–µ 18\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –í–æ–ø—Ä–æ—Å—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. Lars, Lamb. Learning rate schedulers. Warm-up. MultiGPU training.\n\n    –ó–∞–Ω—è—Ç–∏–µ 19\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üë∑‚Äç‚ôÇÔ∏è Seminar\n    \n    –í–≤–µ–¥–µ–Ω–∏–µ –≤ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä—ë–º–∞. –ú–µ—Ç–æ–¥ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞. ADMM.\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/s4_benchmarx_convex.html",
    "href": "notebooks/s4_benchmarx_convex.html",
    "title": "Support Vector Machine",
    "section": "",
    "text": "!pip install numpy matplotlib jax scipy scikit-optimize ucimlrepo optax\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\nArray([  0, 228], dtype=uint32)\n\n\n\n@jit\ndef logistic_loss(w, X, y, mu=1):\n    m, n = X.shape\n    return jnp.sum(jnp.logaddexp(0, -y * (X @ w))) / m + mu / 2 * jnp.sum(w**2)\n\ndef generate_problem(m=1000, n=300, mu=1):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//2, random_state=0)\n    X = jnp.array(X)\n    y = jnp.array(y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n    return X_train, y_train, X_test, y_test\n\ndef compute_optimal(X, y, mu):\n    w = cp.Variable(X.shape[1])\n    objective = cp.Minimize(cp.sum(cp.logistic(cp.multiply(-y, X @ w))) / len(y) + mu / 2 * cp.norm(w, 2)**2)\n    problem = cp.Problem(objective)\n    problem.solve()\n    return w.value, problem.value\n\n@jit\ndef compute_accuracy(w, X, y):\n    # Compute predicted probabilities using the logistic (sigmoid) function\n    preds_probs = jax.nn.sigmoid(X @ w)\n    # Convert probabilities to class predictions: -1 if p &lt; 0.5, else 1\n    preds = jnp.where(preds_probs &lt; 0.5, 0, 1)\n    # Calculate accuracy as the average of correct predictions\n    accuracy = jnp.mean(preds == y)\n    return accuracy\n\n\n\n# @jit\ndef compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu):\n    f = lambda w: logistic_loss(w, X_train, y_train, mu)\n    metrics = {\n        \"f_gap\": [jnp.abs(f(x) - f_star) for x in trajectory],\n        \"x_gap\": [jnp.linalg.norm(x - x_star) for x in trajectory],\n        \"time\": times,\n        \"train_acc\": [compute_accuracy(x, X_train, y_train) for x in trajectory],\n        \"test_acc\": [compute_accuracy(x, X_test, y_test) for x in trajectory],\n    }\n    return metrics\n\ndef gradient_descent(w_0, X, y, learning_rate=0.01, num_iters=100, mu=0):\n    trajectory = [w_0]\n    times = [0]\n    w = w_0\n    f = lambda w: logistic_loss(w, X, y, mu)\n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_val = grad(f)(w)\n        if learning_rate == \"linesearch\":\n            # Simple line search implementation\n            phi = lambda alpha: f(w - alpha*grad_val)\n            result = minimize_scalar(fun=phi, \n                                     bounds=(1e-3, 2e2)\n                              )\n            step_size = result.x\n        else:\n            step_size = learning_rate\n        w -= step_size * grad_val\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    return trajectory, times\n\ndef run_experiments(params):\n    mu = params[\"mu\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X_train, y_train, X_test, y_test = generate_problem(m, n, mu)\n    n_features = X_train.shape[1]  # Number of features\n    params[\"n_features\"] = n_features\n    \n    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n_features, ))\n    x_star, f_star = compute_optimal(X_train, y_train, mu)\n\n    for method in methods:\n        learning_rate = method[\"learning_rate\"]\n        iterations = method[\"iterations\"]\n        trajectory, times = gradient_descent(x_0, X_train, y_train, learning_rate, iterations, mu)\n        label = method[\"method\"] + \" \" + str(learning_rate)\n        results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n\n    return results, params\n\ndef plot_results(results, params):\n    plt.figure(figsize=(11, 5))\n    mu = params[\"mu\"]\n    \n    if mu &gt; 1e-2:\n        plt.suptitle(f\"Strongly convex binary logistic regression. mu={mu}.\")\n    else:\n        plt.suptitle(f\"Convex binary logistic regression. mu={mu}.\")\n\n    plt.subplot(2, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 5)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['f_gap'])\n    plt.xlabel('Time')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 6)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['x_gap'])\n    plt.xlabel('Time')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 7)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"train_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 8)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"test_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    # Place the legend below the plots\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    # Adjust layout to make space for the legend below\n    filename = \"\"\n    for method, metrics in results.items():\n        filename += method\n    filename += f\"_{mu}.pdf\"\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.savefig(filename)\n    plt.show()\n\n&lt;&gt;:133: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:165: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:133: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:165: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_87087/2871042674.py:133: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_87087/2871042674.py:165: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"mu\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-1,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-1,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\n\n\n\n\n\n\n\nparams = {\n    \"mu\": 1e-1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 1e-1,\n            \"iterations\": 900,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 1.1e-1,\n            \"iterations\": 900,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\n\n\n\n\n\n\nNow we also have convergence in terms of distance to the solution, but it does not influence accuracy.\n\nSupport Vector Machine\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\nArray([  0, 228], dtype=uint32)\n\n\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\n# Generate a synthetic binary classification dataset\ndef generate_svm_data(m=1000, n=300):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, \n                                       n_informative=n//2, random_state=42)\n    y = 2 * y - 1  # Convert labels to {-1, 1}\n    return X, y\n\n# Solve SVM using convex optimization\ndef compute_svm_optimal(X, y, C=1.0):\n    m, n = X.shape\n    w = cp.Variable(n)\n    b = cp.Variable()\n    slack = cp.Variable(m)\n\n    # SVM objective: minimize 1/2 ||w||^2 + C * sum(slack)\n    objective = cp.Minimize(0.5 * cp.norm(w, 2)**2 + C * cp.sum(slack))\n\n    # Constraints:  y_i (x_i^T w + b) &gt;= 1 - slack_i,  slack_i &gt;= 0\n    constraints = [\n        cp.multiply(y, X @ w + b) &gt;= 1 - slack,\n        slack &gt;= 0\n    ]\n\n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n\n    return w.value, b.value, problem.value\n\n# Hinge loss function\n@jax.jit\ndef hinge_loss(w, b, X, y, C=1.0):\n    margins = y * (X @ w + b)\n    loss = jnp.maximum(0, 1 - margins)\n    return 0.5 * jnp.sum(w**2) + C * jnp.mean(loss)\n\n# Compute accuracy\n@jax.jit\ndef compute_accuracy(w, b, X, y):\n    preds = jnp.sign(X @ w + b)\n    return jnp.mean(preds == y)\n\n# Perform gradient descent for SVM optimization\ndef gradient_descent_svm(w_0, b_0, X, y, learning_rate=0.01, num_iters=100, C=1.0):\n    trajectory = [w_0]\n    times = [0]\n    w, b = w_0, b_0\n    f = lambda w, b: hinge_loss(w, b, X, y, C)\n    \n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_w, grad_b = grad(f, argnums=(0, 1))(w, b)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    \n    return trajectory, times\n\n# Run SVM Experiments\ndef run_experiments_svm(params):\n    C = params[\"C\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X, y = generate_svm_data(m, n)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    # Standardize data\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    w_0 = jax.random.normal(jax.random.PRNGKey(0), (n,))\n    b_0 = 0.0\n\n    w_star, b_star, f_star = compute_svm_optimal(X_train, y_train, C)\n\n    for method in methods:\n        learning_rate = method[\"learning_rate\"]\n        iterations = method[\"iterations\"]\n        trajectory, times = gradient_descent_svm(w_0, b_0, X_train, y_train, learning_rate, iterations, C)\n        label = method[\"method\"] + \" \" + str(learning_rate)\n        results[label] = {\n            \"f_gap\": [jnp.abs(hinge_loss(w, b_0, X_train, y_train, C) - f_star) for w in trajectory],\n            \"x_gap\": [jnp.linalg.norm(w - w_star) for w in trajectory],\n            \"time\": times,\n            \"train_acc\": [compute_accuracy(w, b_0, X_train, y_train) for w in trajectory],\n            \"test_acc\": [compute_accuracy(w, b_0, X_test, y_test) for w in trajectory],\n        }\n\n    return results, params\n\n# Plot Results\ndef plot_results_svm(results, params):\n    plt.figure(figsize=(11, 5))\n    C = params[\"C\"]\n    \n    plt.suptitle(f\"SVM Training Results. C={C}\")\n\n    plt.subplot(2, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 5)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['f_gap'])\n    plt.xlabel('Time')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 6)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['x_gap'])\n    plt.xlabel('Time')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 7)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"train_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Train accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 8)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"test_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Test accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.show()\n\n&lt;&gt;:119: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:149: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:119: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:149: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_86548/3657982053.py:119: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_86548/3657982053.py:149: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"C\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-3,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-3,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments_svm(params)\nplot_results_svm(results, params)\n\n\n\n\n\n\n\n\nHere we only minimize \\frac{1}{2} ||w||_2^2 without hinge loss, we have linear convergence both for function error and distance to the solution.\n\nparams = {\n    \"C\": 0.1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-3,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-3,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments_svm(params)\nplot_results_svm(results, params)\n\n\n\n\n\n\n\n\nHere we minimize regularized hinge loss and see that we have accuracy improvement."
  },
  {
    "objectID": "notebooks/s1_lora_trump.html",
    "href": "notebooks/s1_lora_trump.html",
    "title": "",
    "section": "",
    "text": "!pip install -q transformers datasets peft accelerate bitsandbytes"
  },
  {
    "objectID": "notebooks/s1_lora_trump.html#train",
    "href": "notebooks/s1_lora_trump.html#train",
    "title": "",
    "section": "Train",
    "text": "Train\n\nimport torch\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, PeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nfrom copy import deepcopy\n\ntorch.random.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7fe5ae6cc250&gt;\n\n\n\n# Load the Trump Tweets dataset\ndataset = load_dataset(\"yunfan-y/trump-tweets-cleaned\")\n\n# Load the tokenizer and the base model\nmodel_name = \"unsloth/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n)\n\n\n# Define a function for generating text\ndef generate_example(prompt, model, tokenizer, max_length=50):\n    model.eval()  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.95\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove retweets\n    tweet = re.sub(r'^RT\\s+', '', tweet)\n    # Remove user @ references and '#' from hashtags\n    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n    # Remove special characters and numbers\n    # tweet = re.sub(r'[^A-Za-z\\d\\s]', '', tweet)\n    # Convert to lowercase\n    return tweet.strip()\n\n# Preprocessing the data\ndef preprocess_function(examples):\n    tweets = examples[\"text\"]  # Use \"text\" instead of \"content\"\n    inputs = [f\"You are Donald Trump writing a tweet about politics. Your tweet: {clean_tweet(tweet)}\" for tweet in tweets]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n    \ntokenizer.pad_token = tokenizer.eos_token\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n# Split the dataset into training and validation sets\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n\n# Generate text before training\nprint(\"=== Text Generation Before Training ===\")\nprompt = \"Immigrants\"\nprint(generate_example(prompt, model, tokenizer))\n\n# Prepare LoRA configuration\nlora_config = LoraConfig(\n    r=64,  # Rank of the LoRA update matrices\n    lora_alpha=32,  # LoRA scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n    lora_dropout=0.1,  # Dropout rate for LoRA\n    bias=\"none\",  # Do not train biases\n    task_type=\"CAUSAL_LM\"  # Task type for causal language modeling\n)\n\n# Wrap the model with PEFT\npeft_model = get_peft_model(model, lora_config)\noutput_dir = \"./trump_lora\"\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=output_dir,  # Directory for saving the model\n    eval_strategy=\"steps\",  # No evaluation dataset\n    logging_steps=10,\n    save_strategy=\"steps\",\n    save_steps=10,\n    learning_rate=3e-4,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=4,\n    num_train_epochs=1,\n    max_steps=100,\n    weight_decay=0.01,\n    gradient_accumulation_steps=16,\n    warmup_steps=100,\n    logging_dir=\"./logs\",\n    fp16=True,\n    report_to='none',\n)\n\n\n# Define the Trainer with eval_dataset\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\npeft_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Generate text after training\nprint(\"=== Text Generation After Training ===\")\nprint(generate_example(prompt, peft_model, tokenizer))\n\nprint(\"Model fine-tuned and saved!\")\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n=== Text Generation Before Training ===\nImmigrants and the American Dream: The Political and Economic Legacy of Early New York\nImmigrants and the American Dream: The Political and Economic Legacy of Early New York by Anthony F. C. Wallace\nEnglish | February 3, 199\n\n\n\n    \n      \n      \n      [100/100 14:01, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n10\n100.107800\n4.980176\n\n\n20\n32.612900\n0.334947\n\n\n30\n5.274200\n0.308010\n\n\n40\n4.525100\n0.257844\n\n\n50\n3.852100\n0.193377\n\n\n60\n2.864500\n0.175147\n\n\n70\n2.653900\n0.171719\n\n\n80\n2.623700\n0.169075\n\n\n90\n2.646900\n0.167437\n\n\n100\n2.612200\n0.165744\n\n\n\n\n\n\nRepo card metadata block was not found. Setting CardData to empty.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n=== Text Generation After Training ===\nImmigrants‚Äô rights advocates are worried that changes to immigration policies are making it harder for people to seek asylum in the United States. And those changes have also made it harder to prove that someone is eligible for asylum, according to the experts interviewed by\nModel fine-tuned and saved!"
  },
  {
    "objectID": "notebooks/s1_lora_trump.html#inference",
    "href": "notebooks/s1_lora_trump.html#inference",
    "title": "",
    "section": "Inference",
    "text": "Inference\nSeems like when we load fine-tuned model, it overwrites original model, so restart notebook just in case before inference\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\ni = 6 # device number to use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n\n\nimport torch\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, PeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nfrom copy import deepcopy\n\ntorch.random.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7f4c68630310&gt;\n\n\n\n# Load the Trump Tweets dataset\ndataset = load_dataset(\"yunfan-y/trump-tweets-cleaned\")\n\n# Load the tokenizer and the base model\nmodel_name = \"unsloth/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n)\n\n\n# Define a function for generating text\ndef generate_example(prompt, model, tokenizer, max_length=50):\n    model.eval()  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.95\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove retweets\n    tweet = re.sub(r'^RT\\s+', '', tweet)\n    # Remove user @ references and '#' from hashtags\n    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n    # Remove special characters and numbers\n    # tweet = re.sub(r'[^A-Za-z\\d\\s]', '', tweet)\n    # Convert to lowercase\n    return tweet.strip()\n\n# Preprocessing the data\ndef preprocess_function(examples):\n    tweets = examples[\"text\"]  # Use \"text\" instead of \"content\"\n    inputs = [f\"You are Donald Trump writing a tweet about politics. Your tweet: {clean_tweet(tweet)}\" for tweet in tweets]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n    \ntokenizer.pad_token = tokenizer.eos_token\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n# Split the dataset into training and validation sets\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n\nOriginal model\n\nfrom torch.utils.data import DataLoader\nimport math\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Evaluate both models on the evaluation dataset\nprint(\"=== Comparing Model Performance on Evaluation Dataset ===\")\n\n# Evaluate original model\nmodel.eval()\neval_loss = 0\neval_steps = 0\neval_data_loader = DataLoader(eval_dataset, batch_size=4, collate_fn=lambda x: {k: torch.tensor([d[k] for d in x]).to(model.device) for k in x[0].keys()})\nwith torch.no_grad():\n    for batch in eval_data_loader:\n        outputs = model(**{k: v for k, v in batch.items() if k != 'labels'}, labels=batch['labels'])\n        eval_loss += outputs.loss.item()\n        eval_steps += 1\noriginal_perplexity = math.exp(eval_loss / eval_steps)\nprint(f\"Original Model Perplexity: {original_perplexity:.2f}\")\nprint(f\"Original Model Loss: {eval_loss:.2f}\")\n\n=== Comparing Model Performance on Evaluation Dataset ===\nOriginal Model Perplexity: 732.55\nOriginal Model Loss: 554.11\n\n\n\ntest_prompts = [\n    \"I think, Donald Trump\",\n    \"I think, Barack Obama\",\n    \"I think, Joe Biden\",\n    \"I think, the Democrats are\",\n    \"We need to\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(3):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump will be the next President of the United States. There is a chance that he won‚Äôt but if he doesn‚Äôt then I‚Äôll be very disappointed. To me, it‚Äôs the only way that America can survive.\nAmerica\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, the newly elected 45th President of the United States has just pulled a fast one on us. I think, he is going to be a great President. I‚Äôm not going to hold my breath, however,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump is the most intelligent President the US ever had, and the current one, Barack Obama is the most stupid one. I know it is hard to believe, but it is a fact. Trump is the most intelligent, because\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama would be the first African American president, who will make history in the USA, but he is not the first Black president. That is actually George Washington Carver, the inventor and scientist who was the first African American president\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama will win the US election. He has a great chance, since the opposition, John McCain, has made a big mistake. He has promised to give the same tax reductions to people with a high income as to those with\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a really nice man. I mean, his wife is really nice too. I mean, I can't think of a better word to describe them. And I like his daughter Malia. She seems really sweet.\n\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden, the vice president, is the strongest candidate that we‚Äôve had in 25 years.\nIf you want to do more than just talk about how terrible you think Donald Trump is, you have to offer a specific vision of\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is the first president in recent history who has the potential to be reelected to the presidency. The reason is he is a person with a sense of humor, he is a good orator, and he has a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden‚Äôs political career has been at the brink of collapse since he was elected Vice President. It seems to me that his political career is going to be over sooner than later. The political environment is changing. His party is losing\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are in trouble this time. They are the ones who have been saying that the Republicans can not be trusted. I think that they have the right to be scared. They are getting very, very, very, very,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are now afraid of losing their super-majority in the House and Senate. And they are now trying to get the votes by offering amnesty to illegal immigrants.\nThe Democrats are now trying to pass amnesty, but the Republicans are\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are going to try and impeach Trump for the things they said they‚Äôd do. But it didn‚Äôt work in 1998 and it won‚Äôt work now. They tried to impeach Bush for lying about Iraq,\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to start doing things differently. We must begin to take care of our planet if we want to pass it on to our children. The way we travel is contributing to global warming and pollution. We need to look at ways to make our\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to think hard about the impact of our choices in life.\nThe life of a 6 year old girl was saved by her mother‚Äôs choice to make a different kind of dinner.\nWe need to think hard about the impact of our choices\n---\nWe need to look at the whole picture and get our minds around it. We need to understand what we are up against. We need to be able to see the big picture and be able to make sense of it.\n---\n\n\n\n\ntest_prompts = [\n    \"The United States\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has been working with its partners in the United Nations Security Council to impose sanctions on the Syrian government for its alleged use of chemical weapons in April against civilians in the town of Khan Sheikhoun in Idlib governorate. The US,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States government is a massive employer, and is always looking for qualified candidates to fill a wide variety of open employment positions in locations across the country. Below you‚Äôll find a Qualification Summary for an active, open job listing from the Department\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States of America is a federal country made up of fifty states and one federal district. It is the world‚Äôs largest industrial, military and economic power, and is the leading global superpower.\nThe United States of America is the third largest\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Postal Service is a government-owned corporation that has been operating under the radar for a long time. It is the sole mail delivery company in the United States and is one of the biggest companies in the world, with more than 1\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is the only country in the world that does not have a national health insurance system. In 2019, 28.5 million people had no health insurance coverage, and in 2021, 37 million people remained uninsured\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is currently ranked No. 28 on the list of the world‚Äôs countries by life expectancy at birth. It is estimated that the US will have a population of 325 million in 2015. Currently, there are nearly \n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is home to an estimated 1.2 million refugees, about half of whom arrived after 9/11. For decades, America has welcomed refugees, granting permanent residence to those fleeing persecution and war. But, in recent years\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a nation with a long history of slavery and the legacy of slavery still affects many people today. One such legacy is the practice of slavery in the United States. This practice has been around for centuries and has had a significant impact\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has been home to a number of notable African Americans, including athletes, politicians, entertainers, and military figures. One of the most famous was Jackie Robinson, who broke the MLB color barrier in 1947. Here are \n---\nThe United States and the rest of the world are facing an unprecedented crisis of misinformation. As the COVID-19 pandemic and other crises have unfolded, millions of people have been bombarded with lies and fake news on social media and other platforms. Mis\n---\n\n\n\n\ntest_prompts = [\n    \"Hillary Clinton is a\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a master of the backroom deal. She's not afraid to reach across the aisle to forge a deal that can get something done. Even though it might not be her ideal, she's willing to compromise to get something done.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a hypocrite for calling for tougher laws to stop sexual harassment in the workplace ‚Äî and she should know better.\nLast week, the former first lady and secretary of state blasted a federal judge who released the record of a settlement between President\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a former U.S. Secretary of State, a former U.S. Senator from New York, and the 67th U.S. Secretary of State. She served as the First Lady of the United States from 1993 to\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a former United States Senator from New York, and the First Lady of the United States. She is a former U.S. Secretary of State and a presidential candidate in the 2008 Democratic primaries. She is running for the Democratic\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a racist. The former secretary of state and presidential hopeful has a long record of promoting racially charged stereotypes that were designed to undermine the civil rights movement.\nBut a recent poll shows that she‚Äôs also a racist among her own party.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a liar. She lies like a rug. The evidence is clear and strong. She lies and then she lies some more. That‚Äôs why she‚Äôs not fit to be president, but that‚Äôs not her only problem. She‚Äôs\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a terrible liar. It‚Äôs not that she‚Äôs a terrible person, it‚Äôs that she‚Äôs a terrible politician. She should have known better than to even try to lie her way out of this one. She‚Äôs so good at\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a \"warmonger\" and should not be president, Donald Trump said on Monday, setting up a likely confrontation with the former secretary of state as the presidential election looms.\nIn a speech at the New York Hilton hotel on\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a dangerous woman\nby Mark Steyn / September 11, 2015 / Leave a comment\nHillary Clinton and her husband Bill Clinton at the 2015 Clinton Global Initiative ¬© Alain Jocard/AFP/Getty Images\nThe\n---\nHillary Clinton is a former U.S. Secretary of State and former First Lady of the United States. A graduate of Yale University and Yale Law School, she served as U.S. Senator for New York from 2001 to 2009,\n---\n\n\n\n\n\nLora model\nMost of the prompts give rather the same results. Probably, I could not find any good prompts for this model. But qualitative results (loss/perplexity on evaluation dataset) speak for itself.\n\n# Evaluate LORA model\noutput_dir = \"./trump_lora\"\npeft_model = PeftModelForCausalLM.from_pretrained(model, output_dir)\npeft_eval_loss = 0\npeft_eval_steps = 0\nwith torch.no_grad():\n    for batch in eval_data_loader:\n        outputs = peft_model(**{k: v for k, v in batch.items() if k != 'labels'}, labels=batch['labels'])\n        peft_eval_loss += outputs.loss.item()\n        peft_eval_steps += 1\npeft_perplexity = math.exp(peft_eval_loss / peft_eval_steps)\nprint(f\"LORA Model Perplexity: {peft_perplexity:.2f}\")\nprint(f\"LORA Model Loss: {peft_eval_loss:.2f}\")\nprint(f\"Perplexity Improvement: {original_perplexity - peft_perplexity:.2f}\")\nprint(f\"Loss improvement: {eval_loss - peft_eval_loss:.2f}\")\n\nLORA Model Perplexity: 1.18\nLORA Model Loss: 13.93\nPerplexity Improvement: 731.37\nLoss improvement: 540.18\n\n\n\ntest_prompts = [\n    \"I think, Donald Trump\",\n    \"I think, Barack Obama\",\n    \"I think, Joe Biden\",\n    \"I think, the Democrats are\",\n    \"We need to\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(3):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, for all his faults, is the most interesting presidential candidate since perhaps William Jennings Bryan. The thing is, I can‚Äôt be sure what Trump is thinking. He‚Äôs an enigma.\nNow, I don‚Äôt want\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, a lot, about politics, and about the country, and about the world, and about everything else. He is a great thinker, and he thinks about everything. The reason he is successful is because he is a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump just won the Presidential election! He will be a great President for our Country! I‚Äôm proud to call myself a Donald Trump fan! He‚Äôs smart, a great businessman, a great TV personality and a great President!\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama, has set a dangerous precedent with his latest tweet regarding politics and the press. Let me explain.\nThere are a lot of writers out there who just write about politics, and they don‚Äôt understand politics at all. They\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a great man, I don't care what his politics are. He's a great man and we need a lot more men like him.\nI'm the best friend you'll ever have. I'm the worst enemy\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a good man. I hope he is reelected as president of the United States. He has done a great job as president of the United States. I hope he will be reelected as president of the\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is a great person and a great politician. He has always been a great president and I think that he has always done a great job of leading the United States and I think that he is a great leader and I think\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is one of the best presidents the United States of America has ever had. He is the man who brought down the Berlin Wall and the Iron Curtain. He helped end the Cold War and the threat of nuclear war.\nHe\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is the best for America. He is a very intelligent and kind person, who really cares about others. He knows what is going on in America and what the best for America is. He has a lot of experience in\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are being too clever by half. They are playing a game of ‚Äúchicken‚Äù with the Republicans. The Republicans have not played the game of ‚Äúchicken‚Äù very well. The Democrats have not played the game of\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are scared of Trump because he is a very strong leader, unlike Obama. He has a lot of political skills and I hope he wins the next election.\nI think, the Democrats are scared of Trump because he is a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are a better party than the Republicans. I would say the Republicans are a joke. And the only thing they really have going is their big money. I think they have the most corrupt politicians in the world. I just\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to protect our environment and the lives of all creatures on this planet. We must protect our planet, our natural resources, our environment, and the animals that inhabit it. We must all take action to save the Earth.\nWe all have\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to have a plan on how to deal with a crisis and the best place to start is with a crisis plan. It is important to know what type of crisis you might be facing and to have a plan in place that can be used\n---\nWe need to protect the environment from pollution by plastic. But what about the plastic inside of us? What is your body made of? Your body is made of different parts and they all serve a different purpose. We need these parts to stay alive\n---\n\n\n\n\ntest_prompts = [\n    \"The United States\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Supreme Court has ruled that federal law does not grant standing to individuals who simply make claims that they were damaged by another party‚Äôs actions. In Spokeo, Inc. v. Robins, the Court explained that ‚Äústanding is\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States‚Äô trade deficit with China was $34.9 billion in the third quarter of 2017, the Commerce Department reported on Tuesday. The trade deficit with China was $375.2 billion in 2017, up from $\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Air Force announced today that the B-52H Stratofortress has earned a top spot on the prestigious 2017 Time Magazine‚Äôs Top 100 of the World‚Äôs Greatest Machines list. The venerable long-range bomber took\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Department of State has announced that it is accepting applications for the Diversity Visa Lottery program for the year 2020. The DV-2020 lottery will offer 55,000 visas for immigrants to the USA who would like to live\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is set to host its first ever esports World Cup in Dallas, Texas at the Dallas Convention Center from August 2nd to 4th. The event will be held in the same venue as the International eSports Federation (iSF\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a nation of immigrants, with people from around the world coming to America in search of a better life. However, the process of immigrating to the United States can be complex and daunting, especially for those who are not familiar\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has become a global superpower as the result of the country's abundant natural resources, efficient markets, and large and motivated population. The United States has a great deal of natural resources, including forests, minerals, oil, and gas\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a country of immigrants, and immigrants have always been an important part of our culture. This is why we are such a diverse and multicultural nation, with people from all over the world coming to make their lives here. If you\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States of America is a federal republic of 50 states and the District of Columbia (DC). The 50 states are the principal political divisions of the country. The District of Columbia is a federal district and is not a state. There\n---\nThe United States is home to many unique attractions, including the Statue of Liberty and the Golden Gate Bridge. The country is also well known for its delicious cuisine, which ranges from the iconic burgers and pizza to the diverse range of international dishes. From\n---\n\n\n\n\ntest_prompts = [\n    \"Hillary Clinton is a\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a bad role model for our children.\nThis should be the biggest concern for all parents with children of voting age, but especially parents who support a candidate for President.\nI am not a Clinton supporter, but I do believe that the\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a good writer. She‚Äôs a good speaker. She‚Äôs a good politician. I don‚Äôt believe she‚Äôs a good person. And if you want to be president you need to be a good person.\nI don‚Äôt know if\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a woman of many contradictions: one of the smartest in the world, and yet a political novice with little experience running for office; a brilliant negotiator, and yet a poor campaigner who has never been able to rally her\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a liar, a hypocrite, and a crook. She has been for years. She is a disgusting person who does not deserve the office of President. She is a weakling who would not be a good leader, she\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a real estate investor and owner of a company with a multimillion-dollar portfolio of real estate investments. Her investments include a building in New York City and in Florida. She has also invested in businesses in the United States and abroad.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a terrible candidate for president of the United States. She is bad on domestic issues. She is bad on foreign issues. She is bad on economics. And she is bad on the Constitution. She would be the worst president we have\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a self-serving politician who uses her position to sell access and influence to donors and special interests, including foreign governments. She is the most corrupt and untrustworthy candidate ever to seek the presidency. She is a liar who has been\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a true fighter and one of the most extraordinary women of all time. Her book Living History is a great read. I‚Äôm sure she will continue to do great things for America and the world.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a major liar and manipulator who would do anything to gain power.\nHer policies are a disaster for America and the world. Her agenda is a threat to our freedoms and the Constitution.\nShe has a history of dishonesty and corruption\n---\nHillary Clinton is a lying scum sucking whore who is only interested in selling her ass for a buck and lining her pockets. I don't know why she is even allowed to run for office. She is a disgrace to the country and should be\n---"
  },
  {
    "objectID": "notebooks/s5_probability_simplex_projection.html",
    "href": "notebooks/s5_probability_simplex_projection.html",
    "title": "",
    "section": "",
    "text": "import time\nimport random\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nHere we can find implementaton of the 1st \\mathcal{O}(n \\log n) algorithm, using Quicksort:\n\ndef project_simplex_sort(y):\n    \"\"\"\n    Projects the vector y onto the unit simplex {x &gt;= 0, sum(x) = 1}.\n    Difficulty: O(n log n).\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    n = len(y)\n    \n    # If the sum is already &lt;= 1 and all coordinates are non-negative,\n    # then this is already a point on the simplex (you need to check).\n    # But the classics usually assume sum(y) &gt;= 1, \n    # nevertheless, we will add protection:\n    if np.all(y &gt;= 0) and np.abs(y.sum() - 1.0) &lt; 1e-12:\n        return y.copy()\n    \n    # Sort in descending order\n    y_sorted = np.sort(y)[::-1]\n    y_cumsum = np.cumsum(y_sorted)\n    \n    # Finding rho\n    # We are looking for the largest k for which y_sorted[k] - (cumsum[k]-1)/(k+1) &gt; 0\n    rho = 0\n    for k in range(n):\n        val = y_sorted[k] - (y_cumsum[k] - 1.0)/(k + 1)\n        if val &gt; 0:\n            rho = k + 1\n    \n    # Counting the theta threshold\n    theta = (y_cumsum[rho - 1] - 1.0) / rho\n    \n    # Building x\n    x = np.maximum(y - theta, 0.0)\n    return x\n\nAnd here we can find an implementation of an algorithm of average complexity \\mathcal{O}(n), but in the worst case \\mathcal{O}(n^2). The idea is that we consistently (as a recursive or iterative approach) search for the ‚Äúpivot‚Äù threshold so that about half of the elements end up on one side of the threshold. Due to randomization, the average work time is obtained \\mathcal{O}(n):\n\ndef project_simplex_linear(y):\n    \"\"\"\n    Projects the vector y onto the unit simplex,\n    using the idea of a quick pivot selection.\n    Average difficulty: O(n).\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    n = len(y)\n    \n    # If the sum is not more than 1 and y &gt;= 0, then it is already in the simplex\n    if np.all(y &gt;= 0) and y.sum() &lt;= 1.0:\n        return y.copy()\n    \n    # Auxiliary function for recursive search\n    def find_pivot_and_sum(indices, current_sum, current_count):\n        if not indices:\n            return current_sum, current_count, [], True\n        \n        # Randomly choosing the index for the pivot\n        pivot_idx = random.choice(indices)\n        pivot_val = y[pivot_idx]\n        \n        # Dividing the elements into &gt;= pivot and &lt; pivot\n        bigger = []\n        smaller = []\n        sum_bigger = 0.0\n        \n        for idx in indices:\n            val = y[idx]\n            if val &gt;= pivot_val:\n                bigger.append(idx)\n                sum_bigger += val\n            else:\n                smaller.append(idx)\n        \n        # Checking to see if we have reached the condition\n        # sum_{waltz&gt;= pivot_val} (val - pivot_val) &lt; 1 ?\n        # Considering that we already have current_sum/current_count\n        new_sum = current_sum + sum_bigger\n        new_count = current_count + len(bigger)\n        \n        # Condition: sum_{v&gt;= pivot} (pivot) = new_sum - new_count * pivot\n        # Compare with 1\n        if (new_sum - new_count * pivot_val) &lt; 1.0:\n            # So pivot_val can still be (or higher)\n            # -&gt; moving towards the \"smaller ones\" (where we can raise the pivot)\n            return (new_sum, new_count, smaller, False)\n        else:\n            # pivot_val is too big, we need to go to the \"big ones\",\n            # i.e. those that are exactly &gt;= pivot, we stay with them\n            # (which may be even higher than the actual threshold).\n            # But pivot_idx itself is also being removed from the proceedings. \n            # (since we know for sure that pivot_val &lt; the true threshold).\n            if pivot_idx in bigger:\n                bigger.remove(pivot_idx)\n                new_sum -= pivot_val\n                new_count -= 1\n            return (current_sum, current_count, bigger, False)\n    \n    indices = list(range(n))\n    s = 0.0    \n    c = 0      \n    while indices:\n        s, c, indices, done = find_pivot_and_sum(indices, s, c)\n        if done:\n            break\n    \n    # When finished, we have \"rho =c\" and \"sum =s\"\n    # theta = (s - 1)/c\n    theta = (s - 1.0)/c\n    x = np.maximum(y - theta, 0)\n    return x\n\nLet‚Äôs generate several large-dimensional vectors (for example, from 10.000 to 500.000) and measure the running time of both simplex projection algorithms:\n\ndef check_projection_simplex(x, tol=1e-9):\n    \"\"\"\n    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ x –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω –Ω–∞ –µ–¥–∏–Ω–∏—á–Ω—ã–π —Å–∏–º–ø–ª–µ–∫—Å:\n    1) x_i &gt;= 0 –¥–ª—è –≤—Å–µ—Ö i\n    2) sum(x_i) ~ 1 (—Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é)\n    \"\"\"\n    if (x &lt; -tol).any():\n        return False\n    s = x.sum()\n    return abs(s - 1.0) &lt; tol\n\ndef generate_dims(start, stop, step):\n    return np.arange(start, stop + step, step).tolist()\n\ndims = generate_dims(10_000, 1_950_000, 50_000)\ntimes_sort = []\ntimes_linear = []\n\nnp.random.seed(42)\n\nfor d in dims:\n    y = np.random.rand(d) * 2.0\n    \n    start = time.perf_counter()\n    x_sort = project_simplex_sort(y)\n    t_sort = time.perf_counter() - start\n    \n    start = time.perf_counter()\n    x_lin = project_simplex_linear(y)\n    t_lin = time.perf_counter() - start\n    \n    times_sort.append(t_sort)\n    times_linear.append(t_lin)\n    \n    assert check_projection_simplex(x_sort), \"Sort-based projection incorrect!\"\n    assert check_projection_simplex(x_lin),  \"Linear-based projection incorrect!\"\n    \n    print(f\"dim={d}, time_sort={t_sort:.4f}s, time_lin={t_lin:.4f}s\")\n\n# –ü–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏\nplt.figure(figsize=(8, 5))\nplt.plot(dims, times_sort, 'o--', label='Sort-based O(n log n)')\nplt.plot(dims, times_linear, 'o--', label='Quickselect-based O(n) (avg)')\nplt.xlabel('Dimension n')\nplt.ylabel('Time (s)')\nplt.title('Comparison of projection time on a simplex (Sort vs Quickselect)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\ndim=10000, time_sort=0.0037s, time_lin=0.0033s\ndim=60000, time_sort=0.0153s, time_lin=0.0085s\ndim=110000, time_sort=0.0227s, time_lin=0.0115s\ndim=160000, time_sort=0.0325s, time_lin=0.0303s\ndim=210000, time_sort=0.0428s, time_lin=0.0373s\ndim=260000, time_sort=0.0510s, time_lin=0.0434s\ndim=310000, time_sort=0.0610s, time_lin=0.0210s\ndim=360000, time_sort=0.0708s, time_lin=0.0269s\ndim=410000, time_sort=0.0802s, time_lin=0.0416s\ndim=460000, time_sort=0.0919s, time_lin=0.0506s\ndim=510000, time_sort=0.0997s, time_lin=0.0847s\ndim=560000, time_sort=0.1141s, time_lin=0.0397s\ndim=610000, time_sort=0.1240s, time_lin=0.1871s\ndim=660000, time_sort=0.1349s, time_lin=0.0911s\ndim=710000, time_sort=0.1449s, time_lin=0.1910s\ndim=760000, time_sort=0.1525s, time_lin=0.1030s\ndim=810000, time_sort=0.1680s, time_lin=0.0513s\ndim=860000, time_sort=0.1702s, time_lin=0.1326s\ndim=910000, time_sort=0.1824s, time_lin=0.1944s\ndim=960000, time_sort=0.1931s, time_lin=0.1624s\ndim=1010000, time_sort=0.2021s, time_lin=0.1996s\ndim=1060000, time_sort=0.2140s, time_lin=0.1413s\ndim=1110000, time_sort=0.2287s, time_lin=0.1847s\ndim=1160000, time_sort=0.2557s, time_lin=0.2943s\ndim=1210000, time_sort=0.2475s, time_lin=0.2495s\ndim=1260000, time_sort=0.2578s, time_lin=0.2290s\ndim=1310000, time_sort=0.2626s, time_lin=0.1429s\ndim=1360000, time_sort=0.2762s, time_lin=0.2241s\ndim=1410000, time_sort=0.2805s, time_lin=0.2289s\ndim=1460000, time_sort=0.2921s, time_lin=0.1103s\ndim=1510000, time_sort=0.3057s, time_lin=0.1602s\ndim=1560000, time_sort=0.3176s, time_lin=0.2236s\ndim=1610000, time_sort=0.3205s, time_lin=0.2146s\ndim=1660000, time_sort=0.3392s, time_lin=0.3725s\ndim=1710000, time_sort=0.3442s, time_lin=0.2719s\ndim=1760000, time_sort=0.3570s, time_lin=0.3431s\ndim=1810000, time_sort=0.3645s, time_lin=0.1760s\ndim=1860000, time_sort=0.3777s, time_lin=0.1566s\ndim=1910000, time_sort=0.4000s, time_lin=0.2485s\ndim=1960000, time_sort=0.4205s, time_lin=0.1840s\n\n\n\n\n\n\n\n\n\nThus, we see that the second algorithm is always superior to the first one on average, but in some few cases the first one is superior to the second one. Apparently, in these cases, the most ‚Äúinconvenient‚Äù cases for the second algorithm are implemented."
  },
  {
    "objectID": "notebooks/s2_brent.html",
    "href": "notebooks/s2_brent.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\n\n# Define two test functions\ndef f1(x):\n    return np.sin(x) + 0.1 * (x ** 2)\n\ndef f2(x):\n    return np.exp(-x**2) + 0.5 * np.cos(3 * x)\n\n# Optimization settings\nmethods = ['brent', 'golden']\nfunctions = [f1, f2]\nfunction_names = [r'$f_1(x) = sin(x) + 0.1 \\cdot x^2$', r'$f_2(x) = exp(-x^2) + 0.5 \\cdot cos(3x)$']\n\n# Store results for plotting\nresults = {}\n\nfor func, func_name in zip(functions, function_names):\n    results[func_name] = {}\n    for method in methods:\n        function_call_values = []\n\n        # Wrap the function to track calls\n        def wrapped_func(x):\n            value = func(x)\n            function_call_values.append(value)\n            return value\n\n        result = minimize_scalar(wrapped_func, method=method)\n\n        # Collect data for plotting\n        results[func_name][method] = {\n            'function_values': function_call_values,\n            'num_calls': len(function_call_values),\n            'x_min': result.x,\n            'fun_min': result.fun\n        }\n\n# Plot results\nfor func_name in function_names:\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\n    # Plot function value vs. iteration number\n    for method in methods:\n        ax.plot(\n            range(len(results[func_name][method]['function_values'])),\n            [fv - results[func_name][method]['fun_min'] for fv in results[func_name][method]['function_values']],\n            label=f'{method.capitalize()} Method'\n        )\n    ax.set_yscale('log')\n    ax.set_title(func_name)\n    ax.set_xlabel('Iteration Number')\n    ax.set_ylabel('Function Value - Optimal Value (log scale)')\n    ax.legend()\n    ax.grid()\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "",
    "section": "",
    "text": "Linear algebra basics\n\n[5 points] Sensitivity Analysis in Linear Systems Consider a nonsingular matrix A \\in \\mathbb{R}^{n \\times n} and a vector b \\in \\mathbb{R}^n. Suppose that due to measurement or computational errors, the vector b is perturbed to \\tilde{b} = b + \\delta b.\n\nDerive an upper bound for the relative error in the solution x of the system Ax = b in terms of the condition number \\kappa(A) and the relative error in b.\n\nProvide a concrete example using a 2 \\times 2 matrix where \\kappa(A) is large (say, \\geq 100500).\n\n[5 points] Effect of Diagonal Scaling on Rank Let A \\in \\mathbb{R}^{n \\times n} be a matrix with rank r. Suppose D \\in \\mathbb{R}^{n \\times n} is a diagonal matrix. Determine the rank of the product DA. Explain your reasoning.\n[8 points] Unexpected SVD Compute the Singular Value Decomposition (SVD) of the following matrices:\n\nA_1 = \\begin{bmatrix} 2 \\\\ 2 \\\\ 8 \\end{bmatrix}\nA_2 = \\begin{bmatrix} 0 & x \\\\ x & 0 \\\\ 0 & 0 \\end{bmatrix}, where x is the sum of your birthdate numbers (day + month).\n\n[10 points] Effect of normalization on rank Assume we have a set of data points x^{(i)}\\in\\mathbb{R}^{n},\\,i=1,\\dots,m, and decide to represent this data as a matrix \nX =\n\\begin{pmatrix}\n  | & & | \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  | & & | \\\\\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times m}.\n\nWe suppose that \\text{rank}\\,X = r.\nIn the problem below, we ask you to find the rank of some matrix M related to X. In particular, you need to find relation between \\text{rank}\\,X = r and \\text{rank}\\,M, e.g., that the rank of M is always larger/smaller than the rank of X or that \\text{rank}\\,M = \\text{rank}\\,X \\big / 35. Please support your answer with legitimate arguments and make the answer as accurate as possible.\nNote that border cases are possible depending on the structure of the matrix X. Make sure to cover them in your answer correctly.\nIn applied statistics and machine learning, data is often normalized. One particularly popular strategy is to subtract the estimated mean \\mu and divide by the square root of the estimated variance \\sigma^2. i.e. \nx \\rightarrow (x - \\mu) \\big / \\sigma.\n After the normalization, we get a new matrix \n\\begin{split}\nY &:=\n\\begin{pmatrix}\n  | & & | \\\\\n  y^{(1)} & \\dots & y^{(m)} \\\\\n  | & & | \\\\\n\\end{pmatrix},\\\\\ny^{(i)} &:= \\frac{x^{(i)} - \\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}}{\\sqrt{\\frac{1}{m}\\sum_{j=1}^{m} \\left(x^{(j)}\\right)^2 - \\left(\\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}\\right)^2}}.\n\\end{split}\n What is the rank of Y if \\text{rank} \\; X = r?\n[20 points] Image Compression with Truncated SVD Explore image compression using Truncated Singular Value Decomposition (SVD). Understand how varying the number of singular values affects the quality of the compressed image. Implement a Python script to compress a grayscale image using Truncated SVD and visualize the compression quality.\n\nTruncated SVD: Decomposes an image A into U, S, and V matrices. The compressed image is reconstructed using a subset of singular values.\nMathematical Representation: \n  A \\approx U_k \\Sigma_k V_k^T\n  \n\nU_k and V_k are the first k columns of U and V, respectively.\n\\Sigma_k is a diagonal matrix with the top k singular values.\nRelative Error: Measures the fidelity of the compressed image compared to the original. \n  \\text{Relative Error} = \\frac{\\| A - A_k \\|}{\\| A \\|}\n  \n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nfrom skimage import io, color\nimport requests\nfrom io import BytesIO\n\ndef download_image(url):\n    response = requests.get(url)\n    img = io.imread(BytesIO(response.content))\n    return color.rgb2gray(img)  # Convert to grayscale\n\ndef update_plot(i, img_plot, error_plot, U, S, V, original_img, errors, ranks, ax1, ax2):\n    # Adjust rank based on the frame index\n    if i &lt; 70:\n        rank = i + 1\n    else:\n        rank = 70 + (i - 69) * 10\n\n    reconstructed_img = ... # YOUR CODE HERE \n\n    # Calculate relative error\n    relative_error = ... # YOUR CODE HERE\n    errors.append(relative_error)\n    ranks.append(rank)\n\n    # Update the image plot and title\n    img_plot.set_data(reconstructed_img)\n    ax1.set_title(f\"Image compression with SVD\\n Rank {rank}; Relative error {relative_error:.2f}\")\n\n    # Remove axis ticks and labels from the first subplot (ax1)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n\n    # Update the error plot\n    error_plot.set_data(ranks, errors)\n    ax2.set_xlim(1, len(S))\n    ax2.grid(linestyle=\":\")\n    ax2.set_ylim(1e-4, 0.5)\n    ax2.set_ylabel('Relative Error')\n    ax2.set_xlabel('Rank')\n    ax2.set_title('Relative Error over Rank')\n    ax2.semilogy()\n\n    # Set xticks to show rank numbers\n    ax2.set_xticks(range(1, len(S)+1, max(len(S)//10, 1)))  # Adjust the step size as needed\n    plt.tight_layout()\n\n    return img_plot, error_plot\n\n\ndef create_animation(image, filename='svd_animation.mp4'):\n    U, S, V = np.linalg.svd(image, full_matrices=False)\n    errors = []\n    ranks = []\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8))\n    img_plot = ax1.imshow(image, cmap='gray', animated=True)\n    error_plot, = ax2.plot([], [], 'r-', animated=True)  # Initial empty plot for errors\n\n    # Add watermark\n    ax1.text(1, 1.02, '@fminxyz', transform=ax1.transAxes, color='gray', va='bottom', ha='right', fontsize=9)\n\n    # Determine frames for the animation\n    initial_frames = list(range(70))  # First 70 ranks\n    subsequent_frames = list(range(70, len(S), 10))  # Every 10th rank after 70\n    frames = initial_frames + subsequent_frames\n\n    ani = animation.FuncAnimation(fig, update_plot, frames=len(frames), fargs=(img_plot, error_plot, U, S, V, image, errors, ranks, ax1, ax2), interval=50, blit=True)\n    ani.save(filename, writer='ffmpeg', fps=8, dpi=300)\n\n    # URL of the image\n    url = \"\"\n\n    # Download the image and create the animation\n    image = download_image(url)\n    create_animation(image)\n\n\n\nConvergence rates\n\n[6 points] Determine (it means to prove the character of convergence if it is convergent) the convergence or divergence of a given sequences\n\nr_{k} = \\frac{1}{\\sqrt{k+5}}.\nr_{k} = 0.101^k.\nr_{k} = 0.101^{2^k}.\n\n[8 points] Let the sequence \\{r_k\\} be defined by \nr_{k+1} =\n\\begin{cases}\n\\frac{1}{2}\\,r_k, & \\text{if } k \\text{ is even}, \\\\\nr_k^2, & \\text{if } k \\text{ is odd},\n\\end{cases}\n with initial value 0 &lt; r_0 &lt; 1. Prove that \\{r_k\\} converges to 0 and analyze its convergence rate. In your answer, determine whether the overall convergence is linear, superlinear, or quadratic.\n[6 points] Determine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, determine whether there is quadratic convergence. \nr_k = \\dfrac{1}{k!}\n\n[8 points] Consider the recursive sequence defined by \nr_{k+1} = \\lambda\\,r_k + (1-\\lambda)\\,r_k^p,\\quad k\\ge0,\n where \\lambda\\in [0,1) and p&gt;1. Which additional conditions on r_0 should be satisfied for the sequence to converge? Show that when \\lambda&gt;0 the sequence converges to 0 with a linear rate (with asymptotic constant \\lambda), and when \\lambda=0 determine the convergence rate in terms of p. In particular, for p=2 decide whether the convergence is quadratic.\n\n\n\nLine search\n\n[10 points] Consider a quadratic function f: \\mathbb{R}^n \\rightarrow \\mathbb{R}, and let us start from a point x_k \\in \\mathbb{R}^n moving in the direction of the antigradient -\\nabla f(x_k), note that \\nabla f(x_k)\\neq 0. Show that the minimum of f along this direction as a function of the step size \\alpha, for a decreasing function at x_k, satisfies Armijo‚Äôs condition for any c_1 in the range 0 \\leq c_1 \\leq \\frac{1}{2}. Specifically, demonstrate that the following inequality holds at the optimal \\alpha^*: \n\\varphi(\\alpha) = f(x_{k+1}) = f(x_k - \\alpha \\nabla f(x_k)) \\leq f(x_k) - c_1 \\alpha \\|\\nabla f(x_k)\\|_2^2\n\nImplementing and Testing Line Search Conditions in Gradient Descent [36 points] \nx_{k+1} = x_k - \\alpha \\nabla f(x_k)\n In this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\nnp.random.seed(214)\n\n# Define the quadratic function and its gradient\ndef quadratic_function(x, A, b):\n    return 0.5 * np.dot(x.T, np.dot(A, x)) - np.dot(b.T, x)\n\ndef grad_quadratic(x, A, b):\n    return np.dot(A, x) - b\n\n# Generate a 2D quadratic problem with a specified condition number\ndef generate_quadratic_problem(cond_number):\n    # Random symmetric matrix\n    M = np.random.randn(2, 2)\n    M = np.dot(M, M.T)\n\n    # Ensure the matrix has the desired condition number\n    U, s, V = np.linalg.svd(M)\n    s = np.linspace(cond_number, 1, len(s))  # Spread the singular values\n    A = np.dot(U, np.dot(np.diag(s), V))\n\n    # Random b\n    b = np.random.randn(2)\n\n    return A, b\n\n# Gradient descent function\ndef gradient_descent(start_point, A, b, stepsize_func, max_iter=100):\n    x = start_point.copy()\n    trajectory = [x.copy()]\n\n    for i in range(max_iter):\n        grad = grad_quadratic(x, A, b)\n        step_size = stepsize_func(x, grad)\n        x -= step_size * grad\n        trajectory.append(x.copy())\n\n    return np.array(trajectory)\n\n# Backtracking line search strategy using scipy\ndef backtracking_line_search(x, grad, A, b, alpha=0.3, beta=0.8):\n    def objective(t):\n        return quadratic_function(x - t * grad, A, b)\n    res = minimize_scalar(objective, method='golden')\n    return res.x\n\n# Generate ill-posed problem\ncond_number = 30\nA, b = generate_quadratic_problem(cond_number)\n\n# Starting point\nstart_point = np.array([1.0, 1.8])\n\n# Perform gradient descent with both strategies\ntrajectory_fixed = gradient_descent(start_point, A, b, lambda x, g: 5e-2)\ntrajectory_backtracking = gradient_descent(start_point, A, b, lambda x, g: backtracking_line_search(x, g, A, b))\n\n# Plot the trajectories on a contour plot\nx1, x2 = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nZ = np.array([quadratic_function(np.array([x, y]), A, b) for x, y in zip(x1.flatten(), x2.flatten())]).reshape(x1.shape)\n\nplt.figure(figsize=(10, 8))\nplt.contour(x1, x2, Z, levels=50, cmap='viridis')\nplt.plot(trajectory_fixed[:, 0], trajectory_fixed[:, 1], 'o-', label='Fixed Step Size')\nplt.plot(trajectory_backtracking[:, 0], trajectory_backtracking[:, 1], 'o-', label='Backtracking Line Search')\n\n# Add markers for start and optimal points\nplt.plot(start_point[0], start_point[1], 'ro', label='Start Point')\noptimal_point = np.linalg.solve(A, b)\nplt.plot(optimal_point[0], optimal_point[1], 'y*', markersize=15, label='Optimal Point')\n\nplt.legend()\nplt.title('Gradient Descent Trajectories on Quadratic Function')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.savefig(\"linesearch.svg\")\nplt.show()\n\n\n\nThe code above plots this\n\n\nStart by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.\n\n[10/36 points] Modify the gradient descent function to include the following line search conditions:\n\nDichotomy\nSufficient Decrease Condition\nWolfe Condition\nPolyak step size \n\\alpha_k = \\frac{f(x_k) - f^*}{\\|\\nabla f(x_k)\\|_2^2},\n where f^* is the optimal value of the function. It seems strange to use the optimal value of the function in the step size, but there are options to estimate it even without knowing the optimal value.\n\nSign Gradient Method: \n\\alpha_k = \\frac{1}{\\|\\nabla f(x_k)\\|_2},\n Test your modified gradient descent algorithm with the implemented step size search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search conditions. Choose and specify the termination criterion. Start from the point x_0 = (-1, 2)^T.\n\n[8/36 points] Compare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.\n[10/36 points] Plot trajectory for another function with the same set of methods \nf(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2\n with x_0 = (-1, 2)^T. You might need to adjust hyperparameters.\n[8/36 points] Plot the same function value from the number of function calls for this experiment.\n\n\n\n\nMatrix calculus\n\n[6 points] Find the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{2}\\Vert A - xx^T\\Vert ^2_F, A \\in \\mathbb{S}^n\n[6 points] Find the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert^2_2.\n[8 points] Find the gradient \\nabla f(x) and hessian f''(x), if \nf(x) = \\frac1m \\sum\\limits_{i=1}^m \\log \\left( 1 + \\exp(a_i^{T}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i, x \\in \\mathbb R^n, \\; \\mu&gt;0\n\n[8 points] Compute the gradient \\nabla_A f(A) of the trace of the matrix exponential function f(A) = \\text{tr}(e^A) with respect to A. Hint: Use the definition of the matrix exponential. Use the definition of the differential df = f(A + dA) - f(A) + o(\\Vert dA \\Vert) with the limit \\Vert dA \\Vert \\to 0.\n[20 points] Principal Component Analysis through gradient calculation. Let there be a dataset \\{x_i\\}_{i=1}^N, x_i \\in \\mathbb{R}^D, which we want to transform into a dataset of reduced dimensionality d using projection onto a linear subspace defined by the matrix P \\in \\mathbb{R}^{D \\times d}. The orthogonal projection of a vector x onto this subspace can be computed as P(P^TP)^{-1}P^Tx. To find the optimal matrix P, consider the following optimization problem: \nF(P) = \\sum_{i=1}^N \\|x_i - P(P^TP)^{-1}P^Tx_i\\|^2 = N \\cdot \\text{tr}\\left((I - P(P^TP)^{-1}P^T)^2 S\\right) \\to \\min_{P \\in \\mathbb{R}^{D \\times d}},\n where S = \\frac{1}{N} \\sum_{i=1}^N x_i x_i^T is the sample covariance matrix for the normalized dataset.\n\nFind the gradient \\nabla_P F(P), calculated for an arbitrary matrix P with orthogonal columns, i.e., P : P^T P = I.\nHint: When calculating the differential dF(P), first treat P as an arbitrary matrix, and then use the orthogonality property of the columns of P in the resulting expression.\nConsider the eigendecomposition of the matrix S: \nS = Q \\Lambda Q^T,\n where \\Lambda is a diagonal matrix with eigenvalues on the diagonal, and Q = [q_1 | q_2 | \\ldots | q_D] \\in \\mathbb{R}^{D \\times D} is an orthogonal matrix consisting of eigenvectors q_i as columns. Prove the following:\n\nThe gradient \\nabla_P F(P) equals zero for any matrix P composed of d distinct eigenvectors q_i as its columns.\nThe minimum value of F(P) is achieved for the matrix P composed of eigenvectors q_i corresponding to the largest eigenvalues of S.\n\n\n\n\n\nAutomatic differentiation and jax\n\nBenchmarking Hessian-Vector Product (HVP) Computation in a Neural Network via JAX [22 points]\nYou are given a simple neural network model (an MLP with several hidden layers using a nonlinearity such as GELU). The model‚Äôs parameters are defined by the weights of its layers. Your task is to compare different approaches for computing the Hessian-vector product (HVP) with respect to the model‚Äôs loss and to study how the computation time scales as the model grows in size.\nModel and Loss Definition: [2/22 points] Here is the code for the model and loss definition. Write a method get_params() that returns the flattened vector of all model weights.\nimport jax\nimport jax.numpy as jnp\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom jax.nn import gelu\n\n# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MLP –º–æ–¥–µ–ª–∏\nclass MLP:\n    def __init__(self, key, layer_sizes):\n        self.layer_sizes = layer_sizes\n        keys = jax.random.split(key, len(layer_sizes) - 1)\n        self.weights = [\n            jax.random.normal(k, (layer_sizes[i], layer_sizes[i + 1]))\n            for i, k in enumerate(keys)\n            ]\n\n    def forward(self, x):\n        for w in self.weights[:-1]:\n            x = gelu(jnp.dot(x, w))\n        return jnp.dot(x, self.weights[-1])\n\n    def get_params(self):\n        ### YOUR CODE HERE ###\n        return None\nHessian and HVP Implementations: [2/22 points] Write a function\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ì–µ—Å—Å–∏–∞–Ω–∞\ndef calculate_hessian(model, params):\n    def loss_fn(p):\n        x = jnp.ones((1, model.layer_sizes[0]))  # –ó–∞–≥–ª—É—à–∫–∞ –≤—Ö–æ–¥–∞\n        return jnp.sum(model.forward(x))\n\n    ### YOUR CODE HERE ###\n    #hessian_fn =           \n    return hessian_fn(params)\nthat computes the full Hessian H of the loss function with respect to the model parameters using JAX‚Äôs automatic differentiation.\nNaive HVP via Full Hessian: [2/22 points] Write a function naive_hvp(hessian, vector) that, given a precomputed Hessian H and a vector v (of the same shape as the parameters), computes the Hessian-vector product using a straightforward matrix-vector multiplication.\nEfficient HVP Using Autograd: [4/22 points] Write a function python   def hvp(f, x, v):       return jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))(x) that directly computes the HVP without explicitly forming the full Hessian. This leverages the reverse-mode differentiation capabilities of JAX.\nTiming Experiment: Consider a family of models with an increasing number of hidden layers.\nns = np.linspace(50, 1000, 15, dtype=int)  # The number of hidden layers\nnum_runs = 10  # The number of runs for averaging\nFor each model configuration:\n\nGenerate the model and extract its parameter vector.\nGenerate a random vector v of the same dimension as the parameters.\nMeasure (do not forget to use .block_until_ready() to ensure accurate timing and proper synchronization) the following:\n\nCombined Time (Full Hessian + Naive HVP): The total time required to compute the full Hessian and then perform the matrix-vector multiplication.\nNaive HVP Time (Excluding Hessian Computation): The time required to perform the matrix-vector multiplication given a precomputed Hessian.\nEfficient HVP Time: The time required to compute the HVP using the autograd-based function.\n\nRepeat each timing measurement for a fixed number of runs (e.g., 10 runs) and record both the mean and standard deviation of the computation times.\n\nVisualization and Analysis: [12/22 points]\n\nPlot the timing results for the three methods on the same graph. For each method, display error bars corresponding to the standard deviation over the runs.\nLabel the axes clearly (e.g., ‚ÄúNumber of Layers‚Äù vs.¬†‚ÄúComputation Time (seconds)‚Äù) and include a legend indicating which curve corresponds to which method.\nAnalyze the scaling behavior. Try analytically derive the scaling of the methods and compare it with the experimental results.\n\n[15 points] We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function f(x) = \\frac{1}{2}\\Vert x\\Vert^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0: \nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n Your goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose that each of 10 components of \\alpha is uniformly distributed on [0; 0.1]. \n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n Choose any constant \\beta and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (\\alpha \\in \\mathbb{R}^{10}) becomes better than it was at the start? How do you check numerically local optimality in this problem?\n\n\n\nConvexity\n\n[10 points] Show that this function is convex.: \nf(x, y, z) = z \\log \\left(e^{\\frac{x}{z}} + e^{\\frac{y}{z}}\\right) + (z - 2)^2 + e^{\\frac{1}{x + y}}\n where the function f : \\mathbb{R}^3 \\to \\mathbb{R} has its domain defined as: \n\\text{dom } f = \\{ (x, y, z) \\in \\mathbb{R}^3 : x + y &gt; 0, \\, z &gt; 0 \\}.\n\n[5 points] The center of mass of a body is an important concept in physics (mechanics). For a system of material points with masses m_i and coordinates x_i, the center of mass is given by: \nx_c = \\frac{\\sum_{i=1}^k m_i x_i}{\\sum_{i=1}^k m_i}\n The center of mass of a body does not always lie inside the body. For example, the center of mass of a doughnut is located in its hole. Prove that the center of mass of a system of material points lies in the convex hull of the set of these points.\n[8 points] Show, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert  = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.\n[5 points] Prove that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\n[8 points] Consider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ‚úÖ or ‚ùé. Explain your answers (with proofs).\n\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\n\n[6 points] Prove that the entropy function, defined as \nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n with \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\n[8 points] Show that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e., \n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen‚Äôs inequality.\n[6 points] Show, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0: \nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\frac{\\mu}{2} \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex."
  }
]